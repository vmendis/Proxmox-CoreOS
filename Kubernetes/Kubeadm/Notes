WIP Notes. These will be beautified later on

Asuumptions:

Familarity with Proxmox
Understanding of Kubnernetes

Part 1
The idea is to create Kubernetes ready build image that can be used to deploy the nodes required
for the Control Plane and Workers.


A. Configueration Files
Link to yaml file:  https://github.com/vmendis/Proxmox-Fedora-CoreOS/blob/main/Kubernetes/bootstrap-kube-software.yaml

This yaml file requires transpiling to an Igntion config file. Use this online service: https://fcct.techoverflow.net/

I am using this ignition file which was created some time ago: https://github.com/vmendis/Proxmox-Fedora-CoreOS/blob/main/Kubernetes/bootstrap-kube-software.ign

I have shortned the github url so I can type it easily on CoreOS using free service: https://y.gy/4Vmj

B. Fedora CoreOS Setup:
Setup a VM on Proxmox for CoreOS.  
      I am suing this automated script: ../Scripts/fedora-core-os-vmbuilder.sh

Boot from the CoreOS LiveCD
   -- Add image of console

cd /tmp
curl https://y.gy/4XmZ  -L -o bootstrap.ign

view bootstrap.ign    // Sanity check the ignition file

lsblk -a
sudo coreos-installer install /dev/sda --ignition-file  bootstrap.ign
sudo init 6

Access the newly created CoreOS using ssh
   $ ssh -i ~/.ssh/core_os_id_rsa core@192.168.1.xx

Let the new server reboot few times.
When the IP number of the new server is visible in Proxmox GUI, it is good to go


C. Kubernetes Software Installation
Scripts to perform Kubernetes setup is pushed using  bootstrap-kube-software.yaml onto the host and
saved in /var/tmp

Once the host is up and running ssh onto it.
cd /var/tmp
./1.sh
./2.sh

Now it is good time either to create a Proxmox template or use this host as the base for other hosts in
the Kubernetes clsuster and clone this as necessary. Choise is yours.


Part 2 Kubernetes Setup
In this part, we are setting up the Kubernetes Cluster. Clone a new host from the Proxmox Template or the host created in Part 1C.

A. Change the IP number and hostname of the server
ssh onto the server
Instructions for changing these are in /var/tmp/0.sh

vi /etc/hostname                             
vi /etc/NetworkManager/system-connections/ens18.nmconnection
init 6

B. Kubernetes configueration
ssh onto the server
cd /var/tmp
./3.sh

As per the screen output, copy the kube-config
exit root shell


Part 3 Verify status of the Kubernetes cluster
kubectl cluster-info
kubectl get po -n kube-system
kubectl get --raw='/readyz?verbose'
kubectl get pods --all-namespaces
kubectl get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName --all-namespaces

We can see the necessary Kubernetes core components running
$ sudo crictl pods
POD ID           CREATED          STATE          NAME                              NAMESPACE         ATTEMPT             RUNTIME
4cc1779cd7307    11 minutes ago   Ready          coredns-5dd5756b68-jt65c          kube-system         0                   (default)
654a9dc94b1b1    11 minutes ago   Ready          coredns-5dd5756b68-tmzjt          kube-system         0                   (default)
b4a40aee2f833    11 minutes ago   Ready          kube-proxy-pj5rs                  kube-system         0                   (default)
4da003bcf5ada    12 minutes ago   Ready          etcd-k8-master1                   kube-system         0                   (default)
5b464632ba520    12 minutes ago   Ready          kube-controller-manager-k8-master1   kube-system      0                   (default)
848aad3e20c90    12 minutes ago   Ready          kube-apiserver-k8-master1         kube-system         0                   (default)
9de2aba91bcc4    12 minutes ago   Ready          kube-scheduler-k8-master1         kube-system         0                   (default)



Part 4 - Setup Networking using Flannel 
sudo sysctl net.bridge.bridge-nf-call-iptables=1
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created


kubectl get pods --all-namespaces
NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-78rxf                1/1     Running   0          26s
kube-system    coredns-5dd5756b68-jt65c             1/1     Running   1          2d23h
kube-system    coredns-5dd5756b68-tmzjt             1/1     Running   1          2d23h
kube-system    etcd-k8-master1                      1/1     Running   2          2d23h
kube-system    kube-apiserver-k8-master1            1/1     Running   1          2d23h
kube-system    kube-controller-manager-k8-master1   1/1     Running   1          2d23h
kube-system    kube-proxy-pj5rs                     1/1     Running   1          2d23h
kube-system    kube-scheduler-k8-master1            1/1     Running   1          2d23h


$ sudo crictl pods
POD ID        CREATED          STATE          NAME                         NAMESPACE           ATTEMPT             RUNTIME
7dee13dd178c3 2 minutes ago    Ready          kube-flannel-ds-78rxf        kube-flannel        0                   (default)
...
...


  
Known Issues:

1. Warning about installing qemu agent
ssh -i ~/.ssh/core_os_id_rsa core@192.168.1.50
Fedora CoreOS 39.20240112.3.0
Tracker: https://github.com/coreos/fedora-coreos-tracker
Discuss: https://discussion.fedoraproject.org/tag/coreos

Last login: Mon Feb  5 03:45:31 2024 from 192.168.1.32
[systemd]
Failed Units: 1
  install-qemu-agent.service

When connecting to the FCOS serever using ssh, message about the failed unit is displayed.
However, the qemu agent has been installed and running. This can be seen on Proxmox as the
IP number of the FCOS is listed.


$ ps -aef|grep agent
root         933       1  0 03:42 ?        00:00:00 /usr/bin/qemu-ga --method=virtio-serial --path=/dev/virtio-ports/org.qemu.guest_agent.0 --blacklist= -F/etc/qemu-ga/fsfreeze-hook

Will try to fix this sometime as the functioanlity is not broken.

2. CoreOS/ Calico is broken!
When I started this project my intention was to use Calico as the Kubernetes CNI.
Calico failed to start and investigation led me to this bug report. Joys of using
OpenSource software!

https://github.com/projectcalico/calico/issues/2712

Both kube-router (https://github.com/cloudnativelabs/kube-router) ("cniVersion":"0.3.0")and
Flannel https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 
is working as of 9.Feb.2024


References:
0. Kubernets
https://kubernetes.io/docs/home/

1 .Kubernetes Architectue
https://spot.io/resources/kubernetes-architecture/11-core-components-explained/


2. Building a kubernetes ready Fedora CoreOS image
From: https://vincentdeborger.be/blog/building-a-kubernetes-ready-fedora-coreos-image/
